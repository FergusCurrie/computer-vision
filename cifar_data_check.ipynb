{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be11f2c-069d-4896-9da5-18e17420ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.data_loader.cifar import get_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3862162-bef3-4b66-9c4b-53dbdb443d8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1417618073.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    fig, axs = plt.subplots(1,4,figsize=(40,10))z\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ank\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "it = iter(cifar)\n",
    "\n",
    "for i in range(10):\n",
    "    images, train_labels = next(it)\n",
    "    fig, axs = plt.subplots(1,4,figsize=(40,10))z\n",
    "    for j in range(4):\n",
    "        image = images[j].detach().cpu()\n",
    "        image = image.permute((1, 2, 0)).numpy()\n",
    "        #image = np.clip(image, 0, 1)\n",
    "        image = (image).astype(np.uint8)\n",
    "        axs[j].imshow(image)\n",
    "        axs[j].set_title(train_labels[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f32e61-0d18-419a-8474-80ab9d85f3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8788c33d-0d14-4392-8de7-5ea357bfc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.convnext_tiny.html#torchvision.models.ConvNeXt_Tiny_Weights\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# from data_loader.cifar import get_cifar\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "CIFAR = Path(\"/home/fergus/data/cifar\")\n",
    "\n",
    "\n",
    "def get_torchvision_cifar():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize images\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=str(CIFAR), transform=transform, train=True, download=True\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2aed99f-f550-4958-8603-023fdb6a8f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader=get_torchvision_cifar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e82079b-7655-4f44-bba4-6ee4da94b55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    inp, label = data\n",
    "    print(inp.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df8f0703-b491-4ac8-b0ce-adf3ab5924c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize images\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc5366b-1a5e-48af-b9fe-fb347775cdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d5ac1-4671-43ba-9e1c-d5c362e2fe62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
